[{"UID":"BkgXHTNtvS","abstract":"We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions.  Let $m$ and $d$ be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size $n\\geq m+2d-2$. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no descent path to global optima. It is known that for $n\\le m$, there always exist descent paths to global optima from all initial weights. In this perspective, our results provide a somewhat sharp characterization of the over-parameterization required for \"existence of descent paths\" in the loss landscape. ","authors":"Arsalan Sharifnassab|Saber Salehkaleybar|S. Jamaloddin Golestani","chat_active":"","code_link":"","keywords":"deep learning theory|loss landscape|optimization|over parameterization|relu networks","paper_link":"","presentation_link":"","title":"Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks"},{"UID":"B1xSperKvH","abstract":"Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The weight update is proportional to the difference in spike timing between the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with our hybrid conversion-and-STDB training perform at $10{\\times}{-}25{\\times}$ fewer number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of $65.19\\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\\times}$ faster compared to converted SNNs with similar accuracy.","authors":"Nitin Rathi|Gopalakrishnan Srinivasan|Priyadarshini Panda|Kaushik Roy","chat_active":"","code_link":"","keywords":"imagenet","paper_link":"","presentation_link":"","title":"Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation"},{"UID":"SJlbGJrtDB","abstract":"We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.","authors":"Junjie LIU|Zhe XU|Runbin SHI|Ray C. C. Cheung|Hayden K.H. So","chat_active":"","code_link":"","keywords":"compression|network compression|neural architecture search|optimization|pruning","paper_link":"","presentation_link":"","title":"Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers"},{"UID":"HkgxW0EYDS","abstract":"We describe a simple and general neural network weight compression approach, in which the network parameters (weights and biases) are represented in a \u201clatent\u201d space, amounting to a reparameterization. This space is equipped with a learned probability model, which is used to impose an entropy penalty on the parameter representation during training, and to compress the representation using a simple arithmetic coder after training. Classification accuracy and model compressibility is maximized jointly, with the bitrate\u2013accuracy trade-off specified by a hyperparameter. We evaluate the method on the MNIST, CIFAR-10 and ImageNet classification benchmarks using six distinct model architectures. Our results show that state-of-the-art model compression can be achieved in a scalable and general way without requiring complex procedures such as multi-stage training.","authors":"Deniz Oktay|Johannes Ball\u00e9|Saurabh Singh|Abhinav Shrivastava","chat_active":"","code_link":"https://ml4health.github.io/2021/","keywords":"compression|computer vision|imagenet|information theory|model compression","paper_link":"https://arxiv.org/pdf/1906.06624.pdf","presentation_link":"38922815","title":"Scalable Model Compression by Entropy Penalized Reparameterization"}]
